# SentinelAI - DSGVO-konforme Lokale KI

<div align="center">
  <h3>ğŸ›¡ï¸ Sichere, Offline-fÃ¤hige KI-Dokumentenanalyse</h3>
  <p>EU AI Act & DSGVO konform â€¢ Mistral NeMo 12B â€¢ 100% DatensouverÃ¤nitÃ¤t</p>
</div>

---

## ğŸ¯ Ãœberblick

SentinelAI ist eine vollstÃ¤ndig lokale KI-LÃ¶sung fÃ¼r Dokumentenanalyse und intelligente Assistenz. Alle Daten bleiben auf Ihrem Rechner - keine Cloud, keine API-Aufrufe, keine Kompromisse bei der Datensicherheit.

### âœ¨ Features

- ğŸ¤– **Lokales LLM**: Mistral NeMo 12B mit 12GB VRAM
- ğŸ“„ **Dokumentenanalyse**: PDF, DOCX, TXT, Markdown
- ğŸ” **RAG-System**: Kontextbasierte Antworten aus Ihren Dokumenten
- ğŸ›¡ï¸ **PII-Erkennung**: Automatische Maskierung sensibler Daten
- ğŸ“Š **Compliance Dashboard**: DSGVO & EU AI Act Ãœbersicht
- ğŸ”’ **Audit Logging**: LÃ¼ckenlose Protokollierung

---

## ğŸš€ Schnellstart

### Voraussetzungen

- Docker & Docker Compose
- NVIDIA GPU mit 12GB+ VRAM (RTX 3060/4070 oder besser)
- NVIDIA Container Toolkit
- 16GB+ RAM empfohlen

### Installation

```bash
# Repository klonen
cd /home/ahmet/Downloads/SentinelAi

# Setup-Skript ausfÃ¼hren
chmod +x scripts/setup.sh
./scripts/setup.sh
```

Das Setup-Skript:
1. âœ… ÃœberprÃ¼ft Docker & GPU
2. âœ… Startet Ollama Container
3. âœ… LÃ¤dt Mistral NeMo 12B herunter (~7GB)
4. âœ… LÃ¤dt Embedding-Modell herunter
5. âœ… Startet alle Services

### Manueller Start

```bash
# Services starten
docker compose up -d

# Logs verfolgen
docker compose logs -f

# Services stoppen
docker compose down
```

### Frontend starten (Entwicklung)

```bash
npm install
npm run dev
```

---

## ğŸ“ Projektstruktur

```
SentinelAi/
â”œâ”€â”€ backend/                 # Python FastAPI Backend
â”‚   â”œâ”€â”€ main.py              # API Endpoints
â”‚   â”œâ”€â”€ services/            # Business Logic
â”‚   â”‚   â”œâ”€â”€ llm_service.py   # Ollama Integration
â”‚   â”‚   â”œâ”€â”€ vector_store.py  # ChromaDB
â”‚   â”‚   â”œâ”€â”€ pii_service.py   # PII Erkennung
â”‚   â”‚   â”œâ”€â”€ document_service.py
â”‚   â”‚   â””â”€â”€ audit_service.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ config.py        # Einstellungen
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ components/              # React Komponenten
â”œâ”€â”€ services/
â”‚   â””â”€â”€ geminiService.ts     # Frontend API Client
â”œâ”€â”€ docker-compose.yml       # Container Orchestrierung
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ setup.sh             # Automatisches Setup
â””â”€â”€ data/                    # Persistente Daten (gitignored)
    â”œâ”€â”€ vectorstore/         # ChromaDB
    â”œâ”€â”€ documents/           # Dokumentmetadaten
    â””â”€â”€ audit/               # Audit Logs
```

---

## ğŸ”§ Konfiguration

### Umgebungsvariablen

Kopieren Sie `.env.example` nach `.env` und passen Sie an:

```bash
# LLM Modell (Standard: Mistral NeMo 12B)
LLM_MODEL=mistral-nemo:12b-instruct-2407-q4_K_M

# FÃ¼r schwÃ¤chere GPUs (8GB VRAM):
LLM_MODEL=mistral:7b-instruct-q4_K_M

# PII-Erkennung deaktivieren
PII_DETECTION_ENABLED=false
```

### Modelle wechseln

```bash
# VerfÃ¼gbare Modelle anzeigen
docker compose exec ollama ollama list

# Alternatives Modell herunterladen
docker compose exec ollama ollama pull llama3.1:8b
```

---

## ğŸ“– API Dokumentation

Nach dem Start verfÃ¼gbar unter: **http://localhost:8000/docs**

### Wichtige Endpoints

| Endpoint | Methode | Beschreibung |
|----------|---------|--------------|
| `/health` | GET | System-Status |
| `/chat` | POST | Chat mit LLM |
| `/chat/stream` | POST | Streaming Chat |
| `/documents/upload` | POST | Dokument hochladen |
| `/documents` | GET | Alle Dokumente |
| `/documents/{id}` | DELETE | Dokument lÃ¶schen (DSGVO) |
| `/compliance/stats` | GET | Compliance Statistiken |
| `/compliance/audit` | GET | Audit Log |

---

## ğŸ”’ Sicherheit & Compliance

### DSGVO KonformitÃ¤t

- âœ… **Keine DatenÃ¼bertragung**: Alle Verarbeitung lokal
- âœ… **Recht auf LÃ¶schung**: DELETE Endpoints implementiert
- âœ… **Audit Trail**: Alle Aktionen werden protokolliert
- âœ… **PII-Schutz**: Automatische Erkennung & Maskierung

### EU AI Act

- âœ… **Risikokategorie**: Minimal (Dokumentenanalyse)
- âœ… **Transparenz**: KI-Nutzung gekennzeichnet
- âœ… **Keine verbotenen Praktiken**

---

## ğŸ› ï¸ Entwicklung

### Backend Tests

```bash
cd backend
pip install -e ".[dev]"
pytest tests/ -v
```

### spaCy Modell installieren

```bash
python -m spacy download de_core_news_lg
```

---

## ğŸ“ Lizenz

MIT License - Siehe [LICENSE](LICENSE)

---

## ğŸ¤ Support

Bei Fragen oder Problemen erstellen Sie ein Issue oder kontaktieren Sie uns unter support@sentinell.ai
